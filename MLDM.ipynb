{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLDM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwdNX019fM9D",
        "outputId": "8ddbc0fe-ecf8-4104-a7fd-b61a6db49500"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6LM04ZCI0m"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/data /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS8bNJIQDPhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a00e2cd-236a-4bbb-ff6d-a494000d738a"
      },
      "source": [
        "!pip install underthesea"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/5f/03ab9091b88e7851aa92da33f8eea6f111423cc1194cf1636c63c1fff3d0/underthesea-1.3.1-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 5.6MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.22.2.post1)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Collecting torch<=1.5.1,>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/cf/007b6de316c9f3d4cb315a60c308342cc299e464167f5ebc369e93b5e23a/torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2MB)\n",
            "\u001b[K     |████████████████████████████████| 753.2MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.41.1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (8.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Collecting transformers<=3.5.1,>=3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2020.12.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<=1.5.1,>=1.1.0->underthesea) (0.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.0.12)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 40.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<=3.5.1,>=3.5.0->underthesea) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers<=3.5.1,>=3.5.0->underthesea) (56.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=512cf89469db5801a8ebb10d30d5d80735ad2c5680d7b760f273c6c54312d8c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: python-crfsuite, seqeval, torch, unidecode, sentencepiece, tokenizers, sacremoses, transformers, underthesea\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed python-crfsuite-0.9.7 sacremoses-0.0.45 sentencepiece-0.1.91 seqeval-1.2.2 tokenizers-0.9.3 torch-1.5.1 transformers-3.5.1 underthesea-1.3.1 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tZ5Xh3GDRT4"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from underthesea import word_tokenize\n",
        "import gensim\n",
        "from sklearn import svm,naive_bayes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXARiumTC-Nf"
      },
      "source": [
        "def getNormalizedData(path):\n",
        "    files = os.listdir(path)\n",
        "    print(files)\n",
        "    X = []\n",
        "    Y = []\n",
        "    for f in files:\n",
        "        label = f.replace('.txt','')\n",
        "        with open(path+'/' + f) as tf:\n",
        "            data = tf.read().split('\\n')[:-1]\n",
        "            data = [' '.join(gensim.utils.simple_preprocess(line)) for line in data]\n",
        "            data = [word_tokenize(line, format=\"text\") for line in data]\n",
        "            # print(data)\n",
        "            X = X + data\n",
        "            Y = Y + [label]*len(data)\n",
        "    return X,Y\n",
        "\n",
        "# X_train,Y_train = getNormalizedData('/content/data/train')\n",
        "# print(len(X_train),len(Y_train))\n",
        "# X_test,Y_test = getNormalizedData('/content/data/test')\n",
        "# pickle.dump(X_train,open('/content/data/X_train.pkl','wb'))\n",
        "# pickle.dump(Y_train,open('/content/data/Y_train.pkl','wb'))\n",
        "# pickle.dump(X_test,open('/content/data/X_test.pkl','wb'))\n",
        "# pickle.dump(Y_test,open('/content/data/Y_test.pkl','wb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGjvK6Az1Yyd"
      },
      "source": [
        "#sửa thành đuờng dẫn chỗ chứa mấy cái file pkl\n",
        "X_train = pickle.load(open('/content/drive/MyDrive/processed_data/X_train.pkl','rb'))\n",
        "Y_train = pickle.load(open('/content/drive/MyDrive/processed_data/Y_train.pkl','rb'))\n",
        "Y_test = pickle.load(open('/content/drive/MyDrive/processed_data/Y_test.pkl','rb'))\n",
        "X_test = pickle.load(open('/content/drive/MyDrive/processed_data/X_test.pkl','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjxHSfZAieOO",
        "outputId": "fbb2cbf9-9dec-4581-f52a-9f04da4d804e"
      },
      "source": [
        "print(X_train[0])\n",
        "print(Y_train[0])\n",
        "print(X_test[0])\n",
        "print(Y_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xuất_hiện trojan bảo_vệ đạo_đức bằng kinh korankhi lây_nhiễm được vào máy_tính con trojan có tên yusufali này có khả_năng quét toàn_bộ các địa_chỉ web được gõ vào thanh địa_chỉ của trình duyệt để tìm ra được hầu_hết những từ_ngữ bằng tiếng anh có liên_quan đến khiêu_dâm khi phát_hiện một địa_chỉ web nào đó có chứa một từ khiêu_dâm là con yusufali sẽ ngay_lập_tức làm ẩn đi minimize cửa_sổ trình duyệt có chứa trang_web đồi_trụy và bật lên một thông_điệp của kinh koran nếu người dùng cứ cố_tình bật lên để xem tiếp thì yusufali sẽ thu nhỏ trang_web một lần nữa và dòng chữ for exit click here sẽ luôn hiện lên kết_quả là người dùng bị nhiễm yusufali sẽ không_thể thưởng_thức được trang web khiêu_dâm đó nhưng tác_hại của yusufali nằm chỗ là nó không_thể phân_biệt đâu là trang web khiêu_dâm hoặc đâu là những trang_web tế giáo_dục hoặc trang_web cho trẻ vị_thành_niên vì hễ bất_kỳ trang_web nào có địa_chỉ chứa những từ như teen sex xxx hoặc một bộ_phận nhạy_cảm nào đó trên cơ_thể người là nó sẽ nhào vô xử_lý ngay_lập_tức trojan yusufali hiện có tốc_độ lây_lan không nhanh công_ty chống virus abingdon của anh khuyên người dùng nên cập_nhật ngay công_cụ chống virus của abingdon là có_thể diệt được con trojan không biết là lợi hay hại này\n",
            "Vi_tinh\n",
            "amd thử_nghiệm chip athlon lõi képhy vọng vượt lên trong cuộc đua hướng tới công_nghệ chip đa lõi trước đối_thủ intel amd vừa trình_làng phiên_bản lõi kép của dòng vi xử_lý athlon dùng cho desktop dự_kiến sản_phẩm này sẽ có_mặt trên thị_trường vào nửa cuối năm nay bộ vi xử_lý lõi kép athlon sẽ hoạt_động tốc_độ ghz với công_suất tối_đa chỉ có sử_dụng transitor kích_thước nm sản_phẩm được sản_xuất tại nhà_máy mới của amd đặt tại dresden đức chúng_tôi đang trong quá_trình sản_xuất và sẽ đưa ra thị_trường vào_khoảng nửa cuối năm nay teresa deonis giám_đốc quản_lý sản_phẩm của amd cho biết khi đó bộ vi xử_lý loại này sẽ có_mặt trên từng pc cũng như trong tay các nhà xây_dựng và phát_triển hệ_thống bộ vi xử_lý đa lõi có_thể chứa hoặc nhiều cpu trên cùng mảnh silicon đây được xem là giải_pháp nhằm giảm mức tiêu_thụ năng_lượng khi máy_tính hoạt_động tốc_độ trên ghz mức này một cpu đơn_thuần sẽ có_thể ngốn hết công_suất nếu sử_dụng bộ vi xử_lý lõi kép năng_lượng tiêu_thụ chỉ tương_đương với cpu đơn khi chạy tốc_độ ghz trong khi hiệu_quả xử_lý lại tăng lên gấp đôi đây là động_thái mới nhất của amd trong cuộc đua song_mã với intel hướng đến công_nghệ đa lõi năm_ngoái cả công_ty bắt_đầu tung ra thông_tin về phiên_bản lõi kép của dòng vi xử_lý hiện_tại đầu tháng intel cho biết sẽ ra_mắt chip pentium loại này vào quý ii đối_với bộ vi xử_lý dành cho server amd cho biết phiên_bản lõi kép của opteron đã sẵn_sàng cho việc sản_xuất đại_trà và sẽ có_mặt trên thị_trường vào giữa năm nay intel thì dự_kiến bổ_sung loại vi xử_lý này cho dòng pentium trong thời_gian tới với chip xeon phiên_bản lõi kép sẽ có_mặt trên thị_trường đầu năm\n",
            "Vi_tinh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IDsdGzrSCoV"
      },
      "source": [
        "dataset_X = X_train + X_test\n",
        "dataset_Y = Y_train + Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4D_saGbSKB_",
        "outputId": "c4d301d3-af4f-47a2-e483-172f9050fe30"
      },
      "source": [
        "len(dataset_X) == len(dataset_Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJkm-uZaSN3W"
      },
      "source": [
        "X_train,X_test,Y_train,Y_test = train_test_split(dataset_X,dataset_Y,test_size = 0.15,random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xohSDi1HZ4Uj",
        "outputId": "c994617d-5f58-4f53-c6aa-5f0de9d7a951"
      },
      "source": [
        "labels = set(Y_train)\n",
        "for label in labels:\n",
        "  print(label+\" : \" +str(Y_train.count(label))+ \"-\" + str(Y_test.count(label)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Phap_luat : 6512-1144\n",
            "Doi_song : 4432-763\n",
            "Van_hoa : 7874-1456\n",
            "The_gioi : 8201-1413\n",
            "Suc_khoe : 7473-1328\n",
            "The_thao : 10193-1772\n",
            "Chinh_tri_Xa_hoi : 10887-1899\n",
            "Kinh_doanh : 6677-1151\n",
            "Khoa_hoc : 3298-618\n",
            "Vi_tinh : 5965-1076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3aq1YcCEO96"
      },
      "source": [
        "#vecto hóa các văn bản dùng tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)#ban đầu là 30000 chiều \n",
        "tfidf_vect.fit(X_train) # learn vocabulary and idf from training set\n",
        "X_data_tfidf =  tfidf_vect.transform(X_train)\n",
        "X_test_tfidf =  tfidf_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL1PKFMmJxgT",
        "outputId": "7b3634b7-58eb-40e5-c5bb-7f8f7c61408e"
      },
      "source": [
        "print(tfidf_vect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=30000,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use_idf=True, vocabulary=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq3vVHGZlDSp"
      },
      "source": [
        "pickle.dump(tfidf_vect,open('tfidf.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paN4K4U4JXW5"
      },
      "source": [
        "pickle.dump(X_data_tfidf,open('tfidfdata.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3TaMesdWLjv"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "#giam so chieu du lieu 30000->300 \n",
        "svd = TruncatedSVD(n_components=300, random_state=42)\n",
        "svd.fit(X_data_tfidf)\n",
        "\n",
        "\n",
        "X_data_tfidf_svd = svd.transform(X_data_tfidf)\n",
        "X_test_tfidf_svd = svd.transform(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jaFDfLpXS55"
      },
      "source": [
        "pickle.dump(svd,open(\"svd.pkl\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSkymu4wWUtH"
      },
      "source": [
        "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3):       \n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
        "    \n",
        "    if is_neuralnet:\n",
        "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
        "        \n",
        "        val_predictions = classifier.predict(X_val)\n",
        "        test_predictions = classifier.predict(X_test)\n",
        "        val_predictions = val_predictions.argmax(axis=-1)\n",
        "        test_predictions = test_predictions.argmax(axis=-1)\n",
        "    else:\n",
        "        classifier.fit(X_train, y_train)\n",
        "    \n",
        "        train_predictions = classifier.predict(X_train)\n",
        "        val_predictions = classifier.predict(X_val)\n",
        "        test_predictions = classifier.predict(X_test)\n",
        "        \n",
        "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
        "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))\n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw4RvFqRW9Ry",
        "outputId": "b9214ba9-a449-4981-ea0d-d7e9ea9ac24c"
      },
      "source": [
        "\n",
        "clf = train_model(svm.SVC(), X_data_tfidf_svd, Y_train, X_test_tfidf_svd, Y_test, is_neuralnet=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy:  0.9246364653243848\n",
            "Test accuracy:  0.9276545166402536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pItYfbFPV7zD",
        "outputId": "a73df4b4-3fb6-46f8-ca50-9b393cd79da0"
      },
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "  \n",
        "# # defining parameter range\n",
        "# param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
        "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "#               'kernel': ['linear']} \n",
        "  \n",
        "# grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)\n",
        "  \n",
        "# # fitting the model for grid search\n",
        "# grid.fit(X_data_tfidf_svd, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
            "[CV] C=0.1, gamma=1, kernel=linear ...................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.902, total=13.7min\n",
            "[CV] C=0.1, gamma=1, kernel=linear ...................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 13.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.900, total=13.6min\n",
            "[CV] C=0.1, gamma=1, kernel=linear ...................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 27.3min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.899, total=13.5min\n",
            "[CV] C=0.1, gamma=1, kernel=linear ...................................\n",
            "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.897, total=13.4min\n",
            "[CV] C=0.1, gamma=1, kernel=linear ...................................\n",
            "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.899, total=13.8min\n",
            "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
            "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.902, total=13.3min\n",
            "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
            "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.900, total=13.1min\n",
            "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
            "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.899, total=13.0min\n",
            "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
            "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.897, total=13.1min\n",
            "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
            "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.899, total=13.3min\n",
            "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
            "[CV] .... C=0.1, gamma=0.01, kernel=linear, score=0.902, total=13.2min\n",
            "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
            "[CV] .... C=0.1, gamma=0.01, kernel=linear, score=0.900, total=13.1min\n",
            "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwoQ92skV-ib"
      },
      "source": [
        "# # print best parameter after tuning\n",
        "# print(grid.best_params_)\n",
        "  \n",
        "# # print how our model looks after hyper-parameter tuning\n",
        "# print(grid.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDJ5Agz5fqdB"
      },
      "source": [
        "#confusion matrix\n",
        "y_pred = clf.predict(X_test_tfidf_svd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLOWxSzFgHFl"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--0m4ekhfwap",
        "outputId": "67f34f7f-af7b-4702-9a41-a7785a975108"
      },
      "source": [
        "naive_bayes_clf = train_model(naive_bayes.MultinomialNB(), X_data_tfidf, Y_train, X_test_tfidf, Y_test, is_neuralnet=False)\n",
        "y_pred_nb = naive_bayes_clf.predict(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy:  0.879334451901566\n",
            "Test accuracy:  0.8835974643423138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtH7zxCkgN9r"
      },
      "source": [
        "y_pred_nb = naive_bayes_clf.predict(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD6afSpYfyOX",
        "outputId": "c65a6044-a65f-42b1-bdd1-6e8d250732a1"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, y_pred_nb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Chinh_tri_Xa_hoi       0.76      0.90      0.83      1899\n",
            "        Doi_song       0.83      0.72      0.77       763\n",
            "        Khoa_hoc       0.97      0.50      0.66       618\n",
            "      Kinh_doanh       0.89      0.88      0.88      1151\n",
            "       Phap_luat       0.94      0.87      0.91      1144\n",
            "        Suc_khoe       0.86      0.93      0.90      1328\n",
            "        The_gioi       0.92      0.91      0.92      1413\n",
            "        The_thao       0.99      0.97      0.98      1772\n",
            "         Van_hoa       0.84      0.95      0.89      1456\n",
            "         Vi_tinh       0.94      0.90      0.92      1076\n",
            "\n",
            "        accuracy                           0.88     12620\n",
            "       macro avg       0.90      0.85      0.86     12620\n",
            "    weighted avg       0.89      0.88      0.88     12620\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9zqt9QXf4fT"
      },
      "source": [
        "pickle.dump(clf,open(\"/content/svm.pkl\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUAq97owN1tO"
      },
      "source": [
        "clf = pickle.load( open(\"/content/svm.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lCrbTvI_qxm"
      },
      "source": [
        "pickle.dump(naive_bayes_clf,open(\"/content/nb.pkl\",'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GIBH9l8cp6S",
        "outputId": "39b17e3f-295b-4c5c-a7df-1e13926d04a8"
      },
      "source": [
        "#inference\n",
        "clf = pickle.load(open('/content/svm.pkl','rb'))\n",
        "def preprocess(document):\n",
        "  document = ' '.join(gensim.utils.simple_preprocess(document))\n",
        "  document = word_tokenize(document, format=\"text\")\n",
        "  print(document)\n",
        "  tfidf_vectorize = tfidf_vect.transform([document])\n",
        "  svd_vect = svd.transform(tfidf_vectorize)\n",
        "  return svd_vect\n",
        "document = \"\"\"\n",
        "Thủ thành Manuel Neuer tự tin với khả năng ngược dòng của Bayern Munich khi đá lượt về tứ kết Champions League với PSG tối 13/4.\n",
        "\n",
        "* PSG - Barca: 2h thứ Tư 14/4, giờ Hà Nội.\n",
        "\n",
        "\"Chúng tôi cần cái đầu lạnh và trái tim nóng. Trước những cầu thủ nhanh nhẹn của PSG, chúng tôi cần tiếp cận hợp lý, với thái độ thi đấu tích cực, ổn định và giàu động lực\", Neuer nói trên trang Bundesliga. \"Chúng tôi cần bình tĩnh, tấn công và gây áp lực cho PSG. Họ có những vấn đề ở hàng thủ. Nếu tạo được nhiều cơ hội như trận lượt đi, chúng tôi có thể ngược dòng\".\n",
        "\n",
        "Bayern dứt điểm 31 lần nhưng chỉ ghi được hai bàn vào lưới PSG ở lượt đi tứ kết trên sân Allianz hôm 7/4. Ảnh: LÉquipe\n",
        "Bayern dứt điểm 31 lần nhưng chỉ ghi được hai bàn vào lưới PSG ở lượt đi tứ kết trên sân Allianz hôm 7/4. Ảnh: L'Équipe.\n",
        "\n",
        "Lượt đi giữa tuần trước, Bayern thua PSG 2-3 ngay trên sân nhà Allianz Arena. Để vào bán kết, họ cần thắng cách biệt một bàn nhưng từ tỷ số 4-3 trở lên, hoặc cách biệt tối thiểu hai bàn trên sân Công viên các Hoàng tử. Tuy nhiên, Bayern vẫn vắng tiền đạo chủ lực Robert Lewandowski do chấn thương. Tiền đạo cánh Serge Gnabry cũng phải nghỉ trận thứ hai liên tiếp ở Champions League do chưa hết Covid-19.\n",
        "\n",
        "Ngoài ra, theo cựu danh thủ Lothar Matthaus, đương kim vô địch Bayern còn gặp vấn đề trong cách vận hành lối chơi. Ông nhận xét: \"Bayern luôn chấp nhận mạo hiểm, nhưng khi mất bóng, họ không còn chắc chắn như mùa trước. Họ cũng thiếu tốc độ và sự khôn khéo\".\n",
        "\n",
        "Trái lại, PSG chỉ có một mối bận tâm về nhân sự là chấn thương của Marquinhos. Trong trận thắng Strassbourg 4-1 tại Ligue I tuần trước, trung vệ thủ quân người Brazil phải ngồi ngoài.\n",
        "\n",
        "Điểm mạnh của PSG là cặp tiền đạo Neymar - Kylian Mbappe. Ở lượt đi, Mbappe lập cú đúp, còn Neymar góp hai kiến tạo. Nếu duy trì được lợi thế trước Bayern, họ sẽ vào bán kết và gặp đội thắng trong cặp Dortmund - Man City. Ở lượt đi, Man City thắng 2-1 trên sân nhà Etihad.\n",
        " \"\"\"\n",
        "test = preprocess(document)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "thủ_thành manuel neuer tự_tin với khả_năng ngược dòng của bayern munich khi đá lượt_về tứ_kết champions league với psg tối psg barca thứ tư giờ hà_nội chúng_tôi cần cái đầu lạnh và trái_tim nóng trước những cầu_thủ nhanh_nhẹn của psg chúng_tôi cần tiếp_cận hợp_lý với thái_độ thi_đấu tích_cực ổn_định và giàu động_lực neuer nói trên trang bundesliga chúng_tôi cần bình_tĩnh tấn_công và gây áp_lực cho psg họ có những vấn_đề hàng thủ nếu tạo được nhiều cơ_hội như trận lượt đi chúng_tôi có_thể ngược dòng bayern dứt_điểm lần nhưng chỉ ghi được hai bàn vào lưới psg lượt đi tứ_kết trên sân allianz hôm ảnh léquipe bayern dứt_điểm lần nhưng chỉ ghi được hai bàn vào lưới psg lượt đi tứ_kết trên sân allianz hôm ảnh équipe lượt đi giữa tuần trước bayern thua psg ngay trên sân_nhà allianz arena để vào bán_kết họ cần thắng cách_biệt một bàn nhưng từ tỷ_số trở_lên hoặc cách_biệt tối_thiểu hai bàn trên sân công_viên các hoàng_tử tuy_nhiên bayern vẫn vắng tiền_đạo chủ_lực robert lewandowski do chấn_thương tiền_đạo cánh serge gnabry cũng phải nghỉ trận thứ hai liên_tiếp champions league do chưa hết covid ngoài_ra theo cựu danh_thủ lothar matthaus đương_kim vô_địch bayern còn gặp vấn_đề trong cách vận_hành lối chơi ông nhận_xét bayern luôn chấp_nhận mạo_hiểm nhưng khi mất bóng họ không còn chắc_chắn như mùa trước họ cũng thiếu tốc_độ và sự khôn_khéo trái_lại psg chỉ có một mối bận_tâm về nhân_sự là chấn_thương của marquinhos trong trận thắng strassbourg tại ligue tuần trước trung_vệ thủ_quân người brazil phải ngồi ngoài điểm mạnh của psg là cặp tiền_đạo neymar kylian mbappe lượt đi mbappe lập cú đúp còn neymar góp hai kiến_tạo nếu duy_trì được lợi_thế trước bayern họ sẽ vào bán_kết và gặp đội thắng trong cặp dortmund man city lượt đi man city thắng trên sân_nhà etihad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgGgQtDnN6bs",
        "outputId": "9ec512c3-6902-45bd-a151-43f75550e4a4"
      },
      "source": [
        "clf.predict(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['The_thao'], dtype='<U16')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzmoLLNXOFdx"
      },
      "source": [
        "!pip freeze"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}